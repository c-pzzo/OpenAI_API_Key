{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "352f1d23",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "485630d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -q openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548a9c69",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c814493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef4e311",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98af5232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads the API key from a local file\n",
    "# <Edit Here the path to your OpenAI API Key location>\n",
    "key_file = pd.read_csv(r'.../path_to_API_Key_location/openai_api.txt', header=None)\n",
    "\n",
    "# Store the key in a variable\n",
    "openai.api_key = key_file[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "349245ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The openai.ChatCompletion.create() function generates a response to a sequence of messages in the context of a conversation. Here are the parameters of the function:\n",
      "\n",
      "* model: The engine that the API will connect to generate the response.\n",
      "\n",
      "* text_prompt: Input text for ChatGPT.\n",
      "\n",
      "* temperature: Float value ∈ [0, 1]. Controls the creativity (i.e. randomness) of the generated text. A higher value means a more creative and unexpected response, and vice versa. \n",
      "\n",
      "* max_tokens: Maximum number of tokens (i.e. words or phrases) for the generated text.\n",
      "\n",
      "* n: An integer specifying the number of top responses to return.\n",
      "\n",
      "* stop: An optional string or list of strings specifying the stopping criteria for the generated response. When the generated text contains any of the specified strings, the response is considered complete and the generation process stops.\n",
      "\n",
      "* messages: A list of dictionaries representing the conversation history. Each dictionary should have two keys: \"role\" (a string representing the role of the message sender) and \"content\" (a string representing the text of the message).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "The openai.ChatCompletion.create() function generates a response to a sequence of messages in the context of a conversation. Here are the parameters of the function:\n",
    "\n",
    "* model: The engine that the API will connect to generate the response.\n",
    "\n",
    "* text_prompt: Input text for ChatGPT.\n",
    "\n",
    "* temperature: Float value ∈ [0, 1]. Controls the creativity (i.e. randomness) of the generated text. A higher value means a more creative and unexpected response, and vice versa. \n",
    "\n",
    "* max_tokens: Maximum number of tokens (i.e. words or phrases) for the generated text.\n",
    "\n",
    "* n: An integer specifying the number of top responses to return.\n",
    "\n",
    "* stop: An optional string or list of strings specifying the stopping criteria for the generated response. When the generated text contains any of the specified strings, the response is considered complete and the generation process stops.\n",
    "\n",
    "* messages: A list of dictionaries representing the conversation history. Each dictionary should have two keys: \"role\" (a string representing the role of the message sender) and \"content\" (a string representing the text of the message).\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd0676c",
   "metadata": {},
   "source": [
    "# > Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a68ac4",
   "metadata": {},
   "source": [
    "## >> Prompting ChatGPT for a single reply.\n",
    "The following code uses the model `gpt-3.5-turbo`. It needs the /v1/chat/completions **endpoint**. If the engine were to be changed for an engine with another type of endpoint, then probably the code won't run. It was tested with \"davinci\" and it didn't run because this engine requires a /chat/completions endpoint. They require different code. Check [here](https://platform.openai.com/docs/models/model-endpoint-compatibility) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a6dff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_openai_api(text_prompt\n",
    "                    , current_task_description = \"API connection test\"\n",
    "                    , tokens = 60\n",
    "                    , n_top_answers = 1\n",
    "                    , stop_criteria = None\n",
    "                    , randomness = 0.5):\n",
    "    \"\"\"\n",
    "    The openai.ChatCompletion.create() function generates a response to a sequence of messages in the context of a conversation. Here are the parameters of the function:\n",
    "\n",
    "    * model: The engine that the API will connect to generate the response.\n",
    "    * text_prompt: Input text for ChatGPT.\n",
    "    * temperature: Float value ∈ [0, 1]. Controls the creativity (i.e. randomness) of the generated text. A higher value means a more creative and unexpected response, and vice versa. \n",
    "    * max_tokens: Maximum number of tokens (i.e. words or phrases) for the generated text.\n",
    "    * n: An integer specifying the number of top responses to return.\n",
    "    * stop: An optional string or list of strings specifying the stopping criteria for the generated response. When the generated text contains any of the specified strings, the response is considered complete and the generation process stops.\n",
    "    * messages: A list of dictionaries representing the conversation history. Each dictionary should have two keys: \"role\" (a string representing the role of the message sender) and \"content\" (a string representing the text of the message).\n",
    "    \"\"\"\n",
    "    \n",
    "    chat_messages = [ {\"role\": \"system\", \"content\": str(current_task_description)} ]\n",
    "\n",
    "    chat_messages.append({\"role\": \"user\", \"content\": text_prompt})\n",
    "    chat = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\"\n",
    "                                        , messages=chat_messages\n",
    "                                        , max_tokens=tokens\n",
    "                                        , n=n_top_answers\n",
    "                                        , stop=stop_criteria\n",
    "                                        , temperature=randomness)\n",
    "\n",
    "    reply = chat.choices[0].message.content\n",
    "    chat_messages.append({\"role\": \"assistant\", \"content\": reply})\n",
    "    return reply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a38404",
   "metadata": {},
   "source": [
    "* text_prompt: Input text for ChatGPT.\n",
    "* model: The engine that the API will connect to.\n",
    "* max_tokens: Maximum number of tokens (i.e. words or phrases) for the generated text.\n",
    "* temperature: value ∈ [0, 1]. Controls the randomness of the generated text. Higher value is more random, and vice versa. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b9feaa",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "* [OpenAI API: Complete Python Tutorial](https://analyzingalpha.com/openai-api-python-tutorial)\n",
    "* [Listing Available OpenAI Models – OpenAI API](https://holypython.com/python-api-tutorial/listing-all-available-openai-models-openai-api/)\n",
    "* [Read JSON file using Python](https://www.geeksforgeeks.org/read-json-file-using-python/)\n",
    "* [Models](https://platform.openai.com/docs/models/gpt-3-5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
