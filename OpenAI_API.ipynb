{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "352f1d23",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "485630d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -q openai\n",
    "#pip install json2html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548a9c69",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c814493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import json\n",
    "\n",
    "from IPython.display import display, Markdown, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef4e311",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98af5232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the API key from a local file\n",
    "key_file = pd.read_csv(r'/home/cesar/Coding/openai_api.txt', header=None)\n",
    "\n",
    "# Store the key in a variable\n",
    "openai.api_key = key_file[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "349245ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "The `openai.ChatCompletion.create()` function generates a response to a sequence of messages in the context of a conversation. \n",
       "The following are the parameters of the function:\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: center;\">\n",
       "    <style>\n",
       "      table {\n",
       "        width: 50%;\n",
       "        border-collapse: collapse;\n",
       "      }\n",
       "      th, td {\n",
       "        padding: 8px;\n",
       "        border-bottom: 1px solid #ddd;\n",
       "      }\n",
       "      th:nth-child(2), td:nth-child(2) {\n",
       "        width: 400px; /* Set the desired width for the second column */\n",
       "      }\n",
       "    </style>\n",
       "\n",
       "    <table>\n",
       "      <tr>\n",
       "        <th>Parameter</th>\n",
       "        <th>Description</th>\n",
       "      </tr>\n",
       "      <tr>\n",
       "        <td>model</td>\n",
       "        <td>The engine that the API will connect to generate the response.</td>\n",
       "      </tr>\n",
       "      <tr>\n",
       "        <td>text_prompt</td>\n",
       "        <td>Input text for ChatGPT.</td>\n",
       "      </tr>\n",
       "      <tr>\n",
       "        <td>temperature</td>\n",
       "        <td>Float value ∈ [0, 1]. Controls the creativity (i.e. randomness) of the generated text. A higher value means a more creative and unexpected response, and vice versa.</td>\n",
       "      </tr>\n",
       "      <tr>\n",
       "        <td>max_tokens</td>\n",
       "        <td>Maximum number of tokens (i.e. words or phrases) for the generated text.</td>\n",
       "      </tr>\n",
       "      <tr>\n",
       "        <td>n</td>\n",
       "        <td>An integer specifying the number of top responses to return.</td>\n",
       "      </tr>\n",
       "      <tr>\n",
       "        <td>stop</td>\n",
       "        <td>An optional string or list of strings specifying the stopping criteria for the generated response. When the generated text contains any of the specified strings, the response is considered complete and the generation process stops.</td>\n",
       "      </tr>\n",
       "    </table>\n",
       "</div>    \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "markdown_text = \"\"\"\n",
    "The `openai.ChatCompletion.create()` function generates a response to a sequence of messages in the context of a conversation. \n",
    "The following are the parameters of the function:\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(markdown_text))\n",
    "\n",
    "css_html_code = \"\"\"\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <style>\n",
    "      table {\n",
    "        width: 50%;\n",
    "        border-collapse: collapse;\n",
    "      }\n",
    "      th, td {\n",
    "        padding: 8px;\n",
    "        border-bottom: 1px solid #ddd;\n",
    "      }\n",
    "      th:nth-child(2), td:nth-child(2) {\n",
    "        width: 400px; /* Set the desired width for the second column */\n",
    "      }\n",
    "    </style>\n",
    "\n",
    "    <table>\n",
    "      <tr>\n",
    "        <th>Parameter</th>\n",
    "        <th>Description</th>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>model</td>\n",
    "        <td>The engine that the API will connect to generate the response.</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>text_prompt</td>\n",
    "        <td>Input text for ChatGPT.</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>temperature</td>\n",
    "        <td>Float value ∈ [0, 1]. Controls the creativity (i.e. randomness) of the generated text. A higher value means a more creative and unexpected response, and vice versa.</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>max_tokens</td>\n",
    "        <td>Maximum number of tokens (i.e. words or phrases) for the generated text.</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>n</td>\n",
    "        <td>An integer specifying the number of top responses to return.</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td>stop</td>\n",
    "        <td>An optional string or list of strings specifying the stopping criteria for the generated response. When the generated text contains any of the specified strings, the response is considered complete and the generation process stops.</td>\n",
    "      </tr>\n",
    "    </table>\n",
    "</div>    \n",
    "\"\"\"\n",
    "\n",
    "# Display or save the HTML code as desired\n",
    "display(HTML(css_html_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd0676c",
   "metadata": {},
   "source": [
    "# > Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a68ac4",
   "metadata": {},
   "source": [
    "## >> Prompting ChatGPT for a single reply.\n",
    "The following code uses the model `gpt-3.5-turbo`. It needs the /v1/chat/completions **endpoint**. If the engine were to be changed for an engine with another type of endpoint, then probably the code won't run. It was tested with \"davinci\" and it didn't run because this engine requires a /chat/completions endpoint. They require different code. Check [here](https://platform.openai.com/docs/models/model-endpoint-compatibility) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e9eb203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_openai_api(text_prompt\n",
    "                    , tokens = 60\n",
    "                    , n_top_answers = 1\n",
    "                    , stop_criteria = None\n",
    "                    , randomness = 0.5):\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": text_prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model = \"gpt-3.5-turbo\"\n",
    "        , messages=messages\n",
    "        , n=n_top_answers\n",
    "        , stop=stop_criteria        \n",
    "        , temperature=randomness # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a6dff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_openai_api2(text_prompt\n",
    "                    , current_task_description = \"API connection test\"\n",
    "                    , tokens = 60\n",
    "                    , n_top_answers = 1\n",
    "                    , stop_criteria = None\n",
    "                    , randomness = 0.5):\n",
    "    \"\"\"\n",
    "    The openai.ChatCompletion.create() function generates a response to a sequence of messages in the context of a conversation. Here are the parameters of the function:\n",
    "\n",
    "    * model: The engine that the API will connect to generate the response.\n",
    "    * text_prompt: Input text for ChatGPT.\n",
    "    * temperature: Float value ∈ [0, 1]. Controls the creativity (i.e. randomness) of the generated text. A higher value means a more creative and unexpected response, and vice versa. \n",
    "    * max_tokens: Maximum number of tokens (i.e. words or phrases) for the generated text.\n",
    "    * n: An integer specifying the number of top responses to return.\n",
    "    * stop: An optional string or list of strings specifying the stopping criteria for the generated response. When the generated text contains any of the specified strings, the response is considered complete and the generation process stops.\n",
    "    * messages: A list of dictionaries representing the conversation history. Each dictionary should have two keys: \"role\" (a string representing the role of the message sender) and \"content\" (a string representing the text of the message).\n",
    "    \"\"\"\n",
    "    \n",
    "    chat_messages = [ {\"role\": \"system\", \"content\": str(current_task_description)} ]\n",
    "\n",
    "    chat_messages.append({\"role\": \"user\", \"content\": text_prompt})\n",
    "    chat = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\"\n",
    "                                        , messages=chat_messages\n",
    "                                        , max_tokens=tokens\n",
    "                                        , n=n_top_answers\n",
    "                                        , stop=stop_criteria\n",
    "                                        , temperature=randomness)\n",
    "\n",
    "    reply = chat.choices[0].message.content\n",
    "    chat_messages.append({\"role\": \"assistant\", \"content\": reply})\n",
    "    return reply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a38404",
   "metadata": {},
   "source": [
    "* text_prompt: Input text for ChatGPT.\n",
    "* model: The engine that the API will connect to.\n",
    "* max_tokens: Maximum number of tokens (i.e. words or phrases) for the generated text.\n",
    "* temperature: value ∈ [0, 1]. Controls the randomness of the generated text. Higher value is more random, and vice versa. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b9feaa",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "* [OpenAI API: Complete Python Tutorial](https://analyzingalpha.com/openai-api-python-tutorial)\n",
    "* [Listing Available OpenAI Models – OpenAI API](https://holypython.com/python-api-tutorial/listing-all-available-openai-models-openai-api/)\n",
    "* [Read JSON file using Python](https://www.geeksforgeeks.org/read-json-file-using-python/)\n",
    "* [Models](https://platform.openai.com/docs/models/gpt-3-5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
